#!/usr/bin/env bash

# Ollama | Get up and running with large language models locally.
# https://github.com/jmorganca/ollama

set -e

PROG="${0##*/}"
DEPS=(ollama dmenu)
PROMPT="ï’¾  Ollama"
OLLAMA_SERVER="127.0.0.1:11434"

function logerr_and_exit {
    local msg="$1"
    printf "%s: %s\n" "$PROG" "$msg" >&2
    exit 1
}

function is_available {
    if ! command -v "$1" >/dev/null; then
        return 1
    fi
    return 0
}

function ollama_is_serve {
    if curl -o /dev/null -sS --head "$OLLAMA_SERVER" 2>/dev/null; then
        return 0
    fi

    return 1
}

function prompt {
    local mesg="$1"
    local chose

    chose=$(echo -e "Yes\nNo" | dmenu -i -p "$PROMPT $mesg>")
    if [[ "$chose" != "Yes" ]]; then
        return 1
    fi

    return 0
}

function run {
    local chose

    if ! prompt "start serve"; then
        return 1
    fi

    setsid -f ollama serve
    return 0
}

function main {
    for dep in "${DEPS[@]}"; do
        if ! is_available "$dep"; then
            logerr_and_exit "$dep not found"
        fi
    done

    if ollama_is_serve; then
        if ! prompt "kill?"; then
            return
        fi

        pkill ollama
        return
    fi

    run
}

# Podman err: rootless netns: mount <path>
mkdir -p "/tmp/void-runtime/containers/networks/rootless-netns/run"
podman start ollama
podman exec -it ollama ollama run llama3.1
